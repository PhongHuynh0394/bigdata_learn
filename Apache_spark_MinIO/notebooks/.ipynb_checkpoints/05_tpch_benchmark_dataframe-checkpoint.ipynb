{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f83d8daf-eeec-485e-ad5b-80955fa9e00a",
   "metadata": {},
   "source": [
    "# Init SparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "121a3dd8-b5db-4d30-9e3a-7366f6397990",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from pyspark import SparkContext, HiveContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5cda7e32-256b-475e-810d-33a0a3df3a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/spark/python/pyspark/sql/context.py:112: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder.appName(\"tpch-benchmark-{}\".format(datetime.today()))\n",
    "        .master(\"spark://spark-master:7077\")      \n",
    "        .getOrCreate())\n",
    "\n",
    "sqlContext = SQLContext(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc652a8b-137f-45e7-9083-15131b5ea8e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bb488d32-bcd8-4fd5-b0be-b6ebe4f1fddf",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5aca3b-7553-4720-ad3b-8172410c8fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dates(df_data):\n",
    "    for col in df_data.columns:\n",
    "        if \"date\" in col.lower():\n",
    "            df_data = df_data.withColumn(col, sf.date_sub(sf.to_date(col, \"dd.MM.yy\"), 365 * 100))\n",
    "    return df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7c3553c-bc1a-4eaa-b632-5239e2dc82e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "base_path = \"s3a://warehouse/tpch_data\"\n",
    "df_lineitem = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\" , True)\n",
    "    .csv(os.path.join(base_path, \"h_lineitem.dsv\"))\n",
    ")\n",
    "df_lineitem = convert_dates(df_lineitem)\n",
    "\n",
    "df_customer = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\" , True)\n",
    "    .csv(os.path.join(base_path, \"h_customer.dsv\"))\n",
    ")\n",
    "df_customer = convert_dates(df_customer)\n",
    "\n",
    "df_order = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\" , True)\n",
    "    .csv(os.path.join(base_path, \"h_order.dsv\"))\n",
    ")\n",
    "df_order = convert_dates(df_order)\n",
    "\n",
    "df_supplier = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\" , True)\n",
    "    .csv(os.path.join(base_path, \"h_supplier.dsv\"))\n",
    ")\n",
    "df_supplier = convert_dates(df_supplier)\n",
    "\n",
    "df_nation = (\n",
    "    spark.read.option(\"delimiter\", \"|\")\n",
    "    .option(\"header\", True)\n",
    "    .option(\"inferSchema\" , True)\n",
    "    .csv(os.path.join(base_path, \"h_nation.dsv\"))\n",
    ")\n",
    "df_nation = convert_dates(df_nation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b954e281-8d55-4e54-97db-cbe62d8f4dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# phần này máy em bị crash không load nổi, nên các bài dưới cũng chưa chạy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5d778f-9a32-465a-9149-96c299a3880f",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60efdf8f-7cda-406e-b73b-7e0a1a8dc6ad",
   "metadata": {},
   "source": [
    "## 1. Group by, Order by\n",
    "\n",
    "```sql\n",
    "select\n",
    "\tl_returnflag,\n",
    "\tl_linestatus,\n",
    "\tsum(l_quantity) as sum_qty,\n",
    "\tsum(l_extendedprice) as sum_base_price,\n",
    "\tsum(l_extendedprice * (1 - l_discount)) as sum_disc_price,\n",
    "\tsum(l_extendedprice * (1 - l_discount) * (1 + l_tax)) as sum_charge,\n",
    "\tavg(l_quantity) as avg_qty,\n",
    "\tavg(l_extendedprice) as avg_price,\n",
    "\tavg(l_discount) as avg_disc,\n",
    "\tcount(*) as count_order\n",
    "from\n",
    "\tlineitem\n",
    "group by\n",
    "\tl_returnflag,\n",
    "\tl_linestatus\n",
    "order by\n",
    "\tl_returnflag,\n",
    "\tl_linestatus;\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e561a2c-d60f-4d0f-9d03-294ea8f2ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your Pyspark DataFrame here...\n",
    "sub = (\n",
    "    df_lineitem.groupBy(\"l_returnflag\", \"l_linestatus\")\n",
    ".agg(sum(\"l_quantity\").alias(\"sum_qty\"),\n",
    "sum(\"l_extendedprice\").alias(\"sum_base_price\"),\n",
    "(sum(\"l_extendedprice\") * (1 - \"l_discount\")).alias(\"sum_disc_price\"),\n",
    "(sum(\"l_extendedprice\") * (1 - \"l_discount\") * (1 + \"l_tax\")).alias(\"sum_charge\"),\n",
    "avg(\"l_quantity\").alias(\"avg_qty\"),\n",
    "avg(\"l_extendedprice\").alias(\"avg_price\"),\n",
    "avg(\"l_discount\").alias(\"avg_disc\"),\n",
    "count(\"*\").alias(\"count_order\"))\n",
    ".orderBy(\"l_returnflag\", \"l_linestatus\")\n",
    ")\n",
    "sub.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba21b2b8-ab71-48b8-980e-121218f94ab8",
   "metadata": {},
   "source": [
    "## 2. Join, Group By, Order By\n",
    "\n",
    "```sql\n",
    "select\n",
    "\tl_orderkey,\n",
    "\tsum(l_extendedprice * (1 - l_discount)) as revenue,\n",
    "\to_orderdate,\n",
    "\to_shippriority\n",
    "from\n",
    "\tcustomer,\n",
    "\torders,\n",
    "\tlineitem\n",
    "where\n",
    "\tc_mktsegment = 'AUTOMOBILE'\n",
    "\tand c_custkey = o_custkey\n",
    "\tand l_orderkey = o_orderkey\n",
    "group by\n",
    "\tl_orderkey,\n",
    "\to_orderdate,\n",
    "\to_shippriority\n",
    "order by\n",
    "\trevenue desc,\n",
    "\to_orderdate\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2642cd-e06d-4243-b2b6-a0c4e3e5f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your Pyspark DataFrame here...\n",
    "df = df_lineitem.join(df_orders, df_lineitem.l_orderkey == df_orders.o_orderkey)\\\n",
    "                 .join(df_customer, df_customer.c_custkey == df_orders.o_custkey)\\\n",
    "                 .filter(df_customer.c_mktsegment == 'AUTOMOBILE')\\\n",
    "                 .groupby(df_lineitem.l_orderkey, df_orders.o_orderdate, df_orders.o_shippriority)\\\n",
    "                 .agg(sum((df_lineitem.l_extendedprice * (1 - df_lineitem.l_discount))).alias(\"revenue\"))\\\n",
    "                 .orderBy(['revenue', 'o_orderdate'], ascending=[False, True])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23488739-0b69-45f6-85be-7fa6f331bdef",
   "metadata": {},
   "source": [
    "## 3. Sub queries, Join, Group by, Order by\n",
    "\n",
    "```sql\n",
    "select\n",
    "\tsupp_nation,\n",
    "\tcust_nation,\n",
    "\tl_year,\n",
    "\tsum(volume) as revenue\n",
    "from\n",
    "\t(\n",
    "\t\tselect\n",
    "\t\t\tn1.n_name as supp_nation,\n",
    "\t\t\tn2.n_name as cust_nation,\n",
    "\t\t\textract(year from l_shipdate) as l_year,\n",
    "\t\t\tl_extendedprice * (1 - l_discount) as volume\n",
    "\t\tfrom\n",
    "\t\t\tsupplier,\n",
    "\t\t\tlineitem,\n",
    "\t\t\torders,\n",
    "\t\t\tcustomer,\n",
    "\t\t\tnation n1,\n",
    "\t\t\tnation n2\n",
    "\t\twhere\n",
    "\t\t\ts_suppkey = l_suppkey\n",
    "\t\t\tand o_orderkey = l_orderkey\n",
    "\t\t\tand c_custkey = o_custkey\n",
    "\t\t\tand s_nationkey = n1.n_nationkey\n",
    "\t\t\tand c_nationkey = n2.n_nationkey\n",
    "\t\t\tand l_shipdate between date '1995-01-01' and date '1996-12-31'\n",
    "\t) as shipping\n",
    "group by\n",
    "\tsupp_nation,\n",
    "\tcust_nation,\n",
    "\tl_year\n",
    "order by\n",
    "\tsupp_nation,\n",
    "\tcust_nation,\n",
    "\tl_year;\n",
    "```    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ae0f9-9705-4768-b965-070188203a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write your Pyspark DataFrame here..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b007ba-2afd-4a6d-ba2d-adde45b6e5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_supplier.join(df_lineitem, df_supplier.SUPPKEY == df_lineitem.SUPPKEY) \\\n",
    "    .join(df_orders, df_lineitem.ORDERKEY == df_orders.ORDERKEY) \\\n",
    "    .join(df_customer, df_orders.CUSTKEY == df_customer.CUSTKEY) \\\n",
    "    .join(df_nation.alias(\"n1\"), df_supplier.NATIONKEY == df_nation.NATIONKEY) \\\n",
    "    .join(df_nation.alias(\"n2\"), df_customer.NATIONKEY == df_nation.NATIONKEY) \\\n",
    "    .filter((df_lineitem.SHIPDATE >= \"1995-01-01\") & (df_lineitem.SHIPDATE <= \"1996-12-31\")) \\\n",
    "    .filter(df_customer.MKTSEGMENT == \"AUTOMOBILE\") \\\n",
    "    .selectExpr(\"n1.N_NAME as supp_nation\", \"n2.N_NAME as cust_nation\", \"l_extendedprice * (1 - l_discount) as volume\", \"l_shipdate\") \\\n",
    "    .withColumn(\"l_year\", year(df_lineitem.SHIPDATE.cast(DateType())))\n",
    "\n",
    "# group by\n",
    "df_revenue = df.groupBy(\"supp_nation\", \"cust_nation\", \"l_year\") \\\n",
    "    .agg(sum(\"volume\").alias(\"revenue\")) \\\n",
    "    .orderBy(\"supp_nation\", \"cust_nation\", \"l_year\")\n",
    "\n",
    "df_revenue.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
